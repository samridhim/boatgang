{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCCGATATGTACCCGGACGTAATCCGCAGAATAAAGTATCTAAAAG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GGTAACGAAGTGGTTACATTATGCTCTTCCGTTTCGGATTGTGAAC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCTTCGTCACGGACGTCCCTCGATTAGACTCGATATGTACCTAACG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATGCTTCTACACGCATTATACAGCCCCGCGTTTTAGTCCTTTTCAG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GGACTAGTTGAGGAACAATGAACTCTTTTCAATTGAAACCGAGTTA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  TCCGATATGTACCCGGACGTAATCCGCAGAATAAAGTATCTAAAAG...  1\n",
       "1  GGTAACGAAGTGGTTACATTATGCTCTTCCGTTTCGGATTGTGAAC...  1\n",
       "2  GCTTCGTCACGGACGTCCCTCGATTAGACTCGATATGTACCTAACG...  1\n",
       "3  ATGCTTCTACACGCATTATACAGCCCCGCGTTTTAGTCCTTTTCAG...  1\n",
       "4  GGACTAGTTGAGGAACAATGAACTCTTTTCAATTGAAACCGAGTTA...  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"/home/samridhi/boatgang/dataset/data.csv\", header=None, skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sequences=df[0]\n",
    "temp_target=df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_i = {\"A\" : 0, \"C\": 1, \"G\": 2, \"T\" : 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(s):\n",
    "    i=0\n",
    "    ohe = np.zeros((len(s), 4))\n",
    "    for k in s:\n",
    "        ohe[i,dict_i[k]] = 1\n",
    "        i+=1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  ...\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 200, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_sequences = np.array([one_hot_encode(s) for s in temp_sequences])\n",
    "print ohe_sequences\n",
    "ohe_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Dropout,Flatten\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the Keras deep learning framework (https://github.com/keras-team/keras) to train a CNN to classify\n",
    "the sequences. The sequences were represented using a one-hot encoding with 4 channels (A, C, G and T). The\n",
    "CNN architecture is as follows: Layer 1 is a convolutional layer with 40 filters of size 19 and ReLU activation\n",
    "operating on one-hot encoded input sequences. Layer 2 is a max pooling layer of pool length 10. Layer 3 is a\n",
    "fully connected layer of size 200 with dropout (p=0.5) and ReLU activation. Layer 4 is a fully connected layer\n",
    "with a sigmoid activation. The model was trained with the Adam optimizer and binary cross-entropy loss until\n",
    "no improvement was seen for 3 epochs on the validation set. The datasets, code and model are available at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 191, 40)           1640      \n",
      "=================================================================\n",
      "Total params: 1,640\n",
      "Trainable params: 1,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=40, kernel_size=10, activation='relu', input_shape=(200,4)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling1D(pool_size=10, strides=10))\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(200,  activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 191, 40)           1640      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 19, 40)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 760)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               152200    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 154,041\n",
      "Trainable params: 154,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(ohe_sequences, temp_target, test_size=0.30, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 0.2258 - acc: 0.8805\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 4s 84us/step - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 3s 78us/step - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 3s 80us/step - loss: 7.6755e-04 - acc: 0.9999\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 3s 78us/step - loss: 1.2692e-04 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 3s 83us/step - loss: 2.9053e-05 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 3s 77us/step - loss: 1.3797e-05 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 3s 78us/step - loss: 7.5737e-06 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 3s 79us/step - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 3s 83us/step - loss: 6.8586e-04 - acc: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fadaa33bf90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_train, labels_train, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 35us/step\n",
      "('Test loss ', 0.003057871212526354)\n",
      "('Test acc ', 0.9995555555555555)\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(data_test, labels_test)\n",
    "print (\"Test loss \", scores[0])\n",
    "print (\"Test acc \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers': [{'class_name': 'Conv1D',\n",
       "   'config': {'activation': 'relu',\n",
       "    'activity_regularizer': None,\n",
       "    'batch_input_shape': (None, 200, 4),\n",
       "    'bias_constraint': None,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1,),\n",
       "    'dtype': 'float32',\n",
       "    'filters': 40,\n",
       "    'kernel_constraint': None,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'distribution': 'uniform',\n",
       "      'mode': 'fan_avg',\n",
       "      'scale': 1.0,\n",
       "      'seed': None}},\n",
       "    'kernel_regularizer': None,\n",
       "    'kernel_size': (10,),\n",
       "    'name': 'conv1d_1',\n",
       "    'padding': 'valid',\n",
       "    'strides': (1,),\n",
       "    'trainable': True,\n",
       "    'use_bias': True}},\n",
       "  {'class_name': 'MaxPooling1D',\n",
       "   'config': {'data_format': 'channels_last',\n",
       "    'name': 'max_pooling1d_1',\n",
       "    'padding': 'valid',\n",
       "    'pool_size': (10,),\n",
       "    'strides': (10,),\n",
       "    'trainable': True}},\n",
       "  {'class_name': 'Flatten',\n",
       "   'config': {'data_format': 'channels_last',\n",
       "    'name': 'flatten_1',\n",
       "    'trainable': True}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'activation': 'relu',\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'distribution': 'uniform',\n",
       "      'mode': 'fan_avg',\n",
       "      'scale': 1.0,\n",
       "      'seed': None}},\n",
       "    'kernel_regularizer': None,\n",
       "    'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'units': 200,\n",
       "    'use_bias': True}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'activation': 'sigmoid',\n",
       "    'activity_regularizer': None,\n",
       "    'bias_constraint': None,\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'bias_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'distribution': 'uniform',\n",
       "      'mode': 'fan_avg',\n",
       "      'scale': 1.0,\n",
       "      'seed': None}},\n",
       "    'kernel_regularizer': None,\n",
       "    'name': 'dense_2',\n",
       "    'trainable': True,\n",
       "    'units': 1,\n",
       "    'use_bias': True}}],\n",
       " 'name': 'sequential_1'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplift\n",
    "import json\n",
    "from deeplift.conversion import kerasapi_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_json_string = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertKerasJSONtoDeepLIFT(kerasJSON_str):\n",
    "    jsonData = json.loads(kerasJSON_str)\n",
    "    layersData = jsonData[\"config\"][\"layers\"]\n",
    "    jsonData[\"config\"] = layersData\n",
    "    return json.dumps(jsonData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_json_string = convertKerasJSONtoDeepLIFT(my_json_string)\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(my_json_string)\n",
    "keras_model_weights = \"my_weights.h5\"\n",
    "keras_model_json = \"model.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "keras_model = model_from_json(open(keras_model_json).read())\n",
    "keras_model.load_weights(keras_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift.layers import NonlinearMxtsMode\n",
    "import deeplift.conversion.kerasapi_conversion as kc\n",
    "reload(deeplift.layers)\n",
    "reload(deeplift.conversion.kerasapi_conversion)\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonlinear_mxts_mode is set to: DeepLIFT_GenomicsDefault\n",
      "For layer 0 the preceding linear layer is preact_0 of type Conv1D;\n",
      "In accordance with nonlinear_mxts_mode=DeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to Rescale\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "For layer 3 the preceding linear layer is preact_3 of type Dense;\n",
      "In accordance with nonlinear_mxts_modeDeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to RevealCancel\n",
      "Heads-up: I assume sigmoid is the output layer, not an intermediate one; if it's an intermediate layer then please bug me and I will implement the grad func\n",
      "For layer 4 the preceding linear layer is preact_4 of type Dense;\n",
      "In accordance with nonlinear_mxts_modeDeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to RevealCancel\n",
      "nonlinear_mxts_mode is set to: Rescale\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "Heads-up: I assume sigmoid is the output layer, not an intermediate one; if it's an intermediate layer then please bug me and I will implement the grad func\n",
      "nonlinear_mxts_mode is set to: RevealCancel\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "Heads-up: I assume sigmoid is the output layer, not an intermediate one; if it's an intermediate layer then please bug me and I will implement the grad func\n",
      "nonlinear_mxts_mode is set to: Gradient\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "Heads-up: I assume sigmoid is the output layer, not an intermediate one; if it's an intermediate layer then please bug me and I will implement the grad func\n",
      "nonlinear_mxts_mode is set to: GuidedBackprop\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "Heads-up: I assume sigmoid is the output layer, not an intermediate one; if it's an intermediate layer then please bug me and I will implement the grad func\n"
     ]
    }
   ],
   "source": [
    "method_to_model = OrderedDict()\n",
    "for method_name, nonlinear_mxts_mode in [\n",
    "    #The genomics default = rescale on conv layers, revealcance on fully-connected\n",
    "    ('rescale_conv_revealcancel_fc', NonlinearMxtsMode.DeepLIFT_GenomicsDefault),\n",
    "    ('rescale_all_layers', NonlinearMxtsMode.Rescale),\n",
    "    ('revealcancel_all_layers', NonlinearMxtsMode.RevealCancel),\n",
    "    ('grad_times_inp', NonlinearMxtsMode.Gradient),\n",
    "    ('guided_backprop', NonlinearMxtsMode.GuidedBackprop)]:\n",
    "    method_to_model[method_name] = kc.convert_model_from_saved_files(\n",
    "        h5_file=keras_model_weights,\n",
    "        json_file=keras_model_json,\n",
    "        nonlinear_mxts_mode=nonlinear_mxts_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('maximum difference in predictions:', 0.0)\n"
     ]
    }
   ],
   "source": [
    "from deeplift.util import compile_func\n",
    "model_to_test = method_to_model['rescale_conv_revealcancel_fc']\n",
    "deeplift_prediction_func = compile_func([model_to_test.get_layers()[0].get_activation_vars()],\n",
    "                                         model_to_test.get_layers()[-1].get_activation_vars())\n",
    "original_model_predictions = keras_model.predict(ohe_sequences, batch_size=200)\n",
    "converted_model_predictions = deeplift.util.run_function_in_batches(\n",
    "                                input_data_list=[ohe_sequences],\n",
    "                                func=deeplift_prediction_func,\n",
    "                                batch_size=200,\n",
    "                                progress_update=None)\n",
    "print(\"maximum difference in predictions:\",np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)))\n",
    "assert np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)) < 10**-5\n",
    "predictions = converted_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling scoring functions\n",
      "Compiling scoring function for: rescale_conv_revealcancel_fc\n",
      "Compiling scoring function for: rescale_all_layers\n",
      "Compiling scoring function for: revealcancel_all_layers\n",
      "Compiling scoring function for: grad_times_inp\n",
      "Compiling scoring function for: guided_backprop\n",
      "Compiling integrated gradients scoring functions\n"
     ]
    }
   ],
   "source": [
    "print(\"Compiling scoring functions\")\n",
    "method_to_scoring_func = OrderedDict()\n",
    "for method,model in method_to_model.items():\n",
    "    print(\"Compiling scoring function for: \"+method)\n",
    "    method_to_scoring_func[method] = model.get_target_contribs_func(find_scores_layer_idx=0,\n",
    "                                                                    target_layer_idx=-2)\n",
    "    \n",
    "#To get a function that just gives the gradients, we use the multipliers of the Gradient model\n",
    "gradient_func = method_to_model['grad_times_inp'].get_target_multipliers_func(find_scores_layer_idx=0,\n",
    "                                                                              target_layer_idx=-2)\n",
    "print(\"Compiling integrated gradients scoring functions\")\n",
    "integrated_gradients10_func = deeplift.util.get_integrated_gradients_function(\n",
    "    gradient_computation_function = gradient_func,\n",
    "    num_intervals=10)\n",
    "method_to_scoring_func['integrated_gradients10'] = integrated_gradients10_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('on method', 'rescale_conv_revealcancel_fc')\n",
      "('on method', 'rescale_all_layers')\n",
      "('on method', 'revealcancel_all_layers')\n",
      "('on method', 'grad_times_inp')\n",
      "('on method', 'guided_backprop')\n",
      "('on method', 'integrated_gradients10')\n"
     ]
    }
   ],
   "source": [
    "background    = OrderedDict([('A', 0.25), ('C', 0.25), ('G', 0.25), ('T', 0.25)])\n",
    "\n",
    "from collections import OrderedDict\n",
    "method_to_task_to_scores = OrderedDict()\n",
    "for method_name, score_func in method_to_scoring_func.items():\n",
    "    print(\"on method\",method_name)\n",
    "    method_to_task_to_scores[method_name] = OrderedDict()\n",
    "    for task_idx in [0,1,2]:\n",
    "        scores = np.array(score_func(\n",
    "                    task_idx=task_idx,\n",
    "                    input_data_list=[ohe_sequences],\n",
    "                    input_references_list=[\n",
    "                     np.array([background['A'],\n",
    "                               background['C'],\n",
    "                               background['G'],\n",
    "                               background['T']])[None,None,:]],\n",
    "                    batch_size=200,\n",
    "                    progress_update=None))\n",
    "        assert scores.shape[2]==4\n",
    "        #The sum over the ACGT axis in the code below is important! Recall that DeepLIFT\n",
    "        # assigns contributions based on difference-from-reference; if\n",
    "        # a position is [1,0,0,0] (i.e. 'A') in the actual sequence and [0.3, 0.2, 0.2, 0.3]\n",
    "        # in the reference, importance will be assigned to the difference (1-0.3)\n",
    "        # in the 'A' channel, (0-0.2) in the 'C' channel,\n",
    "        # (0-0.2) in the G channel, and (0-0.3) in the T channel. You want to take the importance\n",
    "        # on all four channels and sum them up, so that at visualization-time you can project the\n",
    "        # total importance over all four channels onto the base that is actually present (i.e. the 'A'). If you\n",
    "        # don't do this, your visualization will look very confusing as multiple bases will be highlighted at\n",
    "        # every position and you won't know which base is the one that is actually present in the sequence!\n",
    "        scores = np.sum(scores, axis=2)\n",
    "        method_to_task_to_scores[method_name][task_idx] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
